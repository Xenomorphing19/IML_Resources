# -*- coding: utf-8 -*-
"""IML_Q6_IIT2019003.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pMCTkYPXxbvA2r1-TfWjJuQw7H2UkKgE
"""

import numpy as np
import pandas as pd
import copy

def normal_equation(X,Y):
  W = np.transpose(X)
  W = np.dot(W,X)
  # print("determinant: ",np.linalg.det(W));
  W = np.linalg.inv(W)
  W = np.dot(W,np.transpose(X))
  W = np.dot(W,Y)
  return W

def normal_equation_with_regularization(X,Y,lamb):
  W = np.transpose(X)
  W = np.dot(W,X)
  one = np.identity(X.shape[1])
  one[0, 0] = 0
  W = W + lamb*one
  W = np.linalg.inv(W)
  W = np.dot(W,np.transpose(X))
  W = np.dot(W,Y)
  return W

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/IML datasets/Housing Price data set.csv")
df = df.drop(['Unnamed: 0'], axis=1)
m = df.shape[0]
n = 4
X = np.array([np.ones(m, np.float32), df['lotsize'].to_numpy(),df['bedrooms'].to_numpy(), df['bathrms'].to_numpy()]).T
Y = df['price'].to_numpy()
X_train, X_test, Y_train, Y_test = X[:382], X[382:], Y[:382], Y[382:]

"""a).  Normal equations  with  and without regularization and compare their performances in terms of % error in prediction."""

print("Normal Equation without Regularization:")
W = normal_equation(X_train,Y_train)
print()
print("Weights without regularization: ", W)
temp = np.dot(X_test, W)
error = abs(Y_test - temp)/Y_test
error = np.sum(error)/164
print()
print("Error without regularization: ",error*100,"%")
print()

lamb = [0.01,0.1,1,5,10,25,50,90,100]
print("Normal Equation with Regularization:\n")
cnt=1
for l in lamb:
  print(cnt,". ")
  W = normal_equation_with_regularization(X_train,Y_train,l)
  print("Weights with regularization: ", W)
  temp = np.dot(X_test, W)
  error = abs(Y_test - temp)/Y_test
  error = np.sum(error)/164
  print()
  print("Error with regularization(lambda = : ",l,"): ",error*100,"%")
  print()
  cnt+=1

"""b). Design Predictor using Batch Gradient Descent Algorithm, Stochastic Gradient Algorithm and mini batch Gradient Descent algorithms (determining minibatch size is your choice- here it could be 10, 20, 30 etc.) with and without feature scaling and compare their performances in terms of % error in prediction."""

def batch_gradient_descent(X,Y,alpha,itr):
  W = np.zeros((n, 1))
  for i in range(0,itr):
     H = np.dot(W.T, X)
     Z = H - Y
     dW = np.dot(X, Z.T)
     W = W - (alpha*dW)/(X.shape[1])
     J = np.sum(Z**2)/(2*m)
  return W

def scaling(X):
  X1 = copy.deepcopy(X)
  stdv = np.std(X1[:, 0:], axis=0)
  stdv[0] = 1
  mean = np.mean(X1[:, 0:], axis=0)
  mean[0] = 0
  X1[:, 0:] = (X1[:, 0:]-mean) / stdv
  return X1,mean,stdv

def batch_gradient_descent_with_scaling(X_train,Y_train,X_test,Y_test,alpha,itr):
  X_train, MEAN, STDV = scaling(X_train)
  X_test = (X_test - MEAN) / STDV
  X_train, X_test = X_train.T, X_test.T
  W = np.zeros((n, 1))
  for i in range(0, itr):
      H = np.dot(W.T, X_train)
      Z = H - Y_train
      dW = np.dot(X_train, Z.T)
      W = W - (alpha*dW)/(X_train.shape[1])
      J = np.sum(Z**2)/(2*m)
  print("Weights with scaling: \n", W)
  print()
  pred = np.dot(W.T, X_test)
  error = abs(Y_test - pred)/Y_test
  error = np.sum(error, axis=1)/X_test.shape[1]
  print("Error with Scaling: ",error*100,"%")

print("Batch Gradient Descent Without Scaling:")
print()
W = batch_gradient_descent(X_train.T,Y_train,0.000000001, 10000)
print("Weights without scaling:\n", W)
print()
pred = np.dot(W.T, X_test.T)
error = abs(Y_test - pred)/Y_test
error = np.sum(error, axis=1)/X_test.T.shape[1]
print("Error without Scaling: ",error*100,"%")

print("Batch Gradient Descent With Scaling:")
print()
batch_gradient_descent_with_scaling(X_train,Y_train,X_test,Y_test,0.01, 10000)

def stochastic_gradient_descent(X,Y,alpha,itr):
  W = np.zeros((n, 1))
  for k in range(0,itr):
      random = np.random.permutation(X.shape[1])
      X_shuffle,Y_shuffle = X[:,random],Y[random]
      for i in range(0,X.shape[1]):
        x = X_shuffle[:, i].reshape((-1, 1))
        y = Y_shuffle[i]
        H = np.dot(W.T, x)
        Z = H - y
        dW = x*Z
        W = W - alpha*dW
  return W

def stochastic_gradient_descent_with_scaling(X_train,Y_train,X_test,Y_test,alpha,itr):
  X_train, MEAN, STDV = scaling(X_train)
  X_test = (X_test - MEAN) / STDV
  X_train, X_test = X_train.T, X_test.T
  W = np.zeros((n, 1))
  for k in range(0, itr):
      random = np.random.permutation(X_train.shape[1])
      X_shuffle,Y_shuffle = X_train[:,random],Y_train[random]
      for i in range(0,X_train.shape[1]):
        x = X_shuffle[:, i].reshape((-1, 1))
        y = Y_shuffle[i]
        H = np.dot(W.T, x)
        Z = H - y
        dW = x*Z
        W = W - alpha*dW
  print("Weights with scaling: \n", W)
  print()
  pred = np.dot(W.T, X_test)
  error = abs(Y_test - pred)/Y_test
  error = np.sum(error, axis=1)/X_test.shape[1]
  print("Error with Scaling: ",error*100,"%")

print("Stochastic Gradient Descent Without Scaling:")
print()
W = stochastic_gradient_descent(X_train.T,Y_train,0.000000001, 10000)
print("Weights without scaling: \n", W)
print()
pred = np.dot(W.T, X_test.T)
error = abs(Y_test - pred)/Y_test
error = np.sum(error, axis=1)/X_test.T.shape[1]
print("Error without Scaling: ",error*100,"%")

print("Stochastic Gradient Descent With Scaling:")
print()
stochastic_gradient_descent_with_scaling(X_train,Y_train,X_test,Y_test,0.01, 10000)

def create(X, Y, sizes):
    mini_batches = []
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)
    num_batches = X.shape[0] // sizes
    i=0
    for i in range(0,num_batches):
      excerpt = indices[i*sizes:(i+1)*sizes]
      X_mini,Y_mini = X[excerpt],Y[excerpt]
      mini_batches.append((X_mini.T,Y_mini))

    if X.shape[0] % sizes != 0:
      excerpt = indices[i*sizes:(i+1)*sizes]
      X_mini, Y_mini = X[excerpt], Y[excerpt]
      mini_batches.append((X_mini.T, Y_mini))
    return mini_batches

def mini_batch_gradient_descent(X,Y,alpha,itr,sizes):
  W = np.zeros((n, 1))
  for i in range(0, itr):
      mini_batches = create(X.T,Y,sizes)
      for mini_batch in mini_batches:
        X_mini,Y_mini = mini_batch
        H = np.dot(W.T, X_mini)
        Z = H - Y_mini
        dW = np.dot(X_mini,Z.T)
        W = W - (alpha*dW)/X_mini.shape[1]
  return W

def mini_batch_gradient_descent_with_scaling(X_train,Y_train,X_test,Y_test,alpha,itr,sizes):
  X_train, MEAN, STDV = scaling(X_train)
  X_test = (X_test - MEAN) / STDV
  X_train, X_test = X_train.T, X_test.T
  W = np.zeros((n, 1))
  for i in range(0, itr):
      mini_batches = create(X_train.T,Y_train,sizes)
      for mini_batch in mini_batches:
        X_mini,Y_mini = mini_batch
        H = np.dot(W.T, X_mini)
        Z = H - Y_mini
        dW = np.dot(X_mini,Z.T)
        W = W - (alpha*dW)/X_mini.shape[1]
  print("Weights with Scaling: \n", W)
  print()
  pred = np.dot(W.T, X_test)
  error = abs(Y_test - pred)/Y_test
  error = np.sum(error, axis=1)/X_test.shape[1]
  print("Error with Scaling,(Batch Size = ",sizes,"): ",error*100,"%")

batch = [10,20,30,40,50]
cnt=1
print("Mini Batch Gradient Descent With Scaling:")
print()
for b in batch:
  print(cnt,". ")
  W = mini_batch_gradient_descent(X_train.T,Y_train, 0.00000001, 10000, b)
  print("Weights without Scaling: \n", W)
  print()
  pred = np.dot(W.T, X_test.T)
  error = abs(Y_test - pred)/Y_test
  error = np.sum(error, axis=1)/X_test.T.shape[1]
  print("Error without Scaling: ",error*100,"%")
  print()
  mini_batch_gradient_descent_with_scaling(X_train,Y_train,X_test,Y_test, 0.01, 10000, b)
  cnt+=1
  print()